% Encoding: UTF-8

@Article{wann,
  author      = {Adam Gaier and David Ha},
  title       = {Weight Agnostic Neural Networks},
  year        = {2019},
  note        = {\url{http://arxiv.org/pdf/1906.04358v2}},
  abstract    = {Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/},
  date        = {2019-06-11},
  eprint      = {1906.04358v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1906.04358v2:PDF},
  keywords    = {cs.LG, cs.NE, stat.ML},
}

@Article{fl_open_problems,
  author      = {Peter Kairouz and H. Brendan McMahan and Brendan Avent and Aurélien Bellet and Mehdi Bennis and Arjun Nitin Bhagoji and Keith Bonawitz and Zachary Charles and Graham Cormode and Rachel Cummings and Rafael G. L. D'Oliveira and Salim El Rouayheb and David Evans and Josh Gardner and Zachary Garrett and Adrià Gascón and Badih Ghazi and Phillip B. Gibbons and Marco Gruteser and Zaid Harchaoui and Chaoyang He and Lie He and Zhouyuan Huo and Ben Hutchinson and Justin Hsu and Martin Jaggi and Tara Javidi and Gauri Joshi and Mikhail Khodak and Jakub Konečný and Aleksandra Korolova and Farinaz Koushanfar and Sanmi Koyejo and Tancrède Lepoint and Yang Liu and Prateek Mittal and Mehryar Mohri and Richard Nock and Ayfer Özgür and Rasmus Pagh and Mariana Raykova and Hang Qi and Daniel Ramage and Ramesh Raskar and Dawn Song and Weikang Song and Sebastian U. Stich and Ziteng Sun and Ananda Theertha Suresh and Florian Tramèr and Praneeth Vepakomma and Jianyu Wang and Li Xiong and Zheng Xu and Qiang Yang and Felix X. Yu and Han Yu and Sen Zhao},
  title       = {Advances and Open Problems in Federated Learning},
  year        = {2019},
  note        = {\url{http://arxiv.org/pdf/1912.04977v1}},
  abstract    = {Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.},
  date        = {2019-12-10},
  eprint      = {1912.04977v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1912.04977v1:PDF},
  keywords    = {cs.LG, cs.CR, stat.ML},
}

@Article{li_fl_future,
  author      = {Tian Li and Anit Kumar Sahu and Ameet Talwalkar and Virginia Smith},
  title       = {Federated Learning: Challenges, Methods, and Future Directions},
  year        = {2019},
  note        = {\url{http://arxiv.org/pdf/1908.07873v1}},
  abstract    = {Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.},
  date        = {2019-08-21},
  eprint      = {1908.07873v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1908.07873v1:PDF},
  keywords    = {cs.LG, cs.DC, stat.ML},
}

@Article{hierarchical_repr,
  author      = {Hanxiao Liu and Karen Simonyan and Oriol Vinyals and Chrisantha Fernando and Koray Kavukcuoglu},
  title       = {Hierarchical Representations for Efficient Architecture Search},
  year        = {2017},
  note        = {\url{https://arxiv.org/pdf/1711.00436v2}},
  abstract    = {We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.},
  date        = {2017-11-01},
  eprint      = {1711.00436v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1711.00436v2:PDF},
  keywords    = {cs.LG, cs.CV, cs.NE, stat.ML},
}

@Article{hochreiter,
  author  = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  title   = {Long Short-term Memory},
  journal = {Neural computation},
  year    = {1997},
  volume  = {9},
  pages   = {1735-80},
  month   = {12},
  note    = {\url{https://www.researchgate.net/publication/13853244_Long_Short-term_Memory/link/5700e75608aea6b7746a0624/download}},
  doi     = {10.1162/neco.1997.9.8.1735},
  file    = {:https\://www.researchgate.net/publication/13853244_Long_Short-term_Memory/link/5700e75608aea6b7746a0624/download:URL},
}

@Article{graves_ntm,
  author      = {Alex Graves and Greg Wayne and Ivo Danihelka},
  title       = {Neural Turing Machines},
  year        = {2014},
  note        = {\url{http://arxiv.org/abs/1410.5401v2}},
  abstract    = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  date        = {2014-10-20},
  eprint      = {1410.5401v2},
  eprintclass = {cs.NE},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1410.5401v2:PDF},
  keywords    = {cs.NE},
}

@Article{arjovsky_uni,
  author      = {Martin Arjovsky and Amar Shah and Yoshua Bengio},
  title       = {Unitary Evolution Recurrent Neural Networks},
  year        = {2015},
  note        = {\url{https://arxiv.org/abs/1511.06464}},
  journal   = {CoRR},
  abstract    = {Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies.},
  date        = {2015-11-20},
  eprint      = {1511.06464v4},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1511.06464v4:PDF},
  keywords    = {cs.LG, cs.NE, stat.ML},
}

@Article{henaff_orth,
  author      = {Mikael Henaff and Arthur Szlam and Yann LeCun},
  title       = {Recurrent Orthogonal Networks and Long-Memory Tasks},
  year        = {2016},
  note        = {\url{http://arxiv.org/abs/1602.06662v2}},
  abstract    = {Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.},
  date        = {2016-02-22},
  eprint      = {1602.06662v2},
  eprintclass = {cs.NE},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1602.06662v2:PDF},
  keywords    = {cs.NE, cs.AI, cs.LG, stat.ML},
}

@Article{cheng_refined,
  author      = {Zhanzhan Cheng and Yunlu Xu and Mingjian Cheng and Yu Qiao and Shiliang Pu and Yi Niu and Fei Wu},
  title       = {Refined Gate: A Simple and Effective Gating Mechanism for Recurrent Units},
  year        = {2020},
  note        = {\url{http://arxiv.org/pdf/2002.11338v1}},
  abstract    = {Recurrent neural network (RNN) has been widely studied in sequence learning tasks, while the mainstream models (e.g., LSTM and GRU) rely on the gating mechanism (in control of how information flows between hidden states). However, the vanilla gates in RNN (e.g. the input gate in LSTM) suffer from the problem of gate undertraining mainly due to the saturating activation functions, which may result in failures of learning gating roles and thus the weak performance. In this paper, we propose a new gating mechanism within general gated recurrent neural networks to handle this issue. Specifically, the proposed gates directly short connect the extracted input features to the outputs of vanilla gates, denoted as refined gates. The refining mechanism allows enhancing gradient back-propagation as well as extending the gating activation scope, which, although simple, can guide RNN to reach possibly deeper minima. We verify the proposed gating mechanism on three popular types of gated RNNs including LSTM, GRU and MGU. Extensive experiments on 3 synthetic tasks, 3 language modeling tasks and 5 scene text recognition benchmarks demonstrate the effectiveness of our method.},
  date        = {2020-02-26},
  eprint      = {2002.11338v1},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/2002.11338v1:PDF},
  keywords    = {cs.CV, cs.LG, cs.NE},
}

@Article{rnn_review,
  author      = {Zachary C. Lipton and John Berkowitz and Charles Elkan},
  title       = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
  year        = {2015},
  note        = {\url{https://arxiv.org/abs/1506.00019}},
  abstract    = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.},
  date        = {2015-05-29},
  eprint      = {1506.00019v4},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1506.00019v4:PDF},
  keywords    = {cs.LG, cs.NE},
}

@Article{elsken_nas_survey,
  author       = {Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},
  title        = {Neural Architecture Search: A Survey},
  year         = {2018},
  abstract     = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
  date         = {2018-08-16},
  eprint       = {1808.05377v3},
  eprintclass  = {stat.ML},
  eprinttype   = {arXiv},
  file         = {online:http\://arxiv.org/pdf/1808.05377v3:PDF},
  journaltitle = {Journal of Machine Learning Research 20 (2019) 1-21},
  keywords     = {stat.ML, cs.LG, cs.NE},
}

@Comment{jabref-meta: databaseType:bibtex;}
